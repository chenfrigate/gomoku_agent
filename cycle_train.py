# cycle_train.py
import os
import argparse
import torch
from config import (
    PRETRAINED_MODEL, GAMES_PER_CYCLE, EPOCHS_PER_CYCLE,
    MCTS_PLAYOUTS
)
from network import YourModelClass
from self_play import self_play_games
from train import train_agent



def load_model(path, device):
    model = YourModelClass().to(device)
    if path and os.path.exists(path):
        # ç›´æ¥å½“æˆå®Œæ•´ checkpoint å¤„ç†ï¼Œå–å‡º 'model_state' å­—å…¸
        ckpt = torch.load(path, map_location=device)
        state_dict = ckpt['model_state']
        model.load_state_dict(state_dict)  # å¦‚æœ‰éœ€è¦å¯åŠ  strict=False
        print(f"[*] Loaded model weights from {path}")
    else:
        print(f"[!] No pretrained model found at {path}, initializing randomly.")
    return model

def cycle_training_loop(
    num_cycles,
    games_per_cycle,
    epochs_per_cycle,
    device,
    white_strategy='expert',  # 'expert', 'latest', 'none'
    winrate_threshold=0.6
):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼š
      - é»‘æ–¹æ¨¡å‹æ¯è½®è‡ªè®­ç»ƒå¹¶æ›´æ–°ï¼›
      - ç™½æ–¹æ¨¡å‹å¯åŸºäºç­–ç•¥ ('expert','latest','none') åˆå§‹ï¼Œå¹¶åœ¨èƒœç‡é˜ˆå€¼ååŠ¨æ€åˆ‡æ¢ã€‚
    """
    expert_model = load_model(PRETRAINED_MODEL, device)

    import glob, re

    # å…ˆå°è¯•å» models/ ç›®å½•é‡Œæ‰¾æ‰€æœ‰ RL å¾ªç¯ä¿å­˜çš„ .pth
    all_rl = glob.glob("models/model_cycle*.pth")
    if all_rl:
        # ä»æ–‡ä»¶åé‡Œæå– cycle æ•°å­—ï¼ŒæŒ‰æ•°å­—å¤§å°æ’åºï¼Œå–æœ€æ–°é‚£ä¸€ä¸ª
        def cycle_num(path):
            m = re.search(r"cycle(\d+)", path)
            return int(m.group(1)) if m else -1

        all_rl.sort(key=cycle_num)
        cur_model_path = all_rl[-1]
        print(f"[*] Found latest RL model: {cur_model_path}")
    else:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°±ç”¨ä¸“å®¶é¢„è®­ç»ƒ
        cur_model_path = PRETRAINED_MODEL if os.path.exists(PRETRAINED_MODEL) else None
        print(f"[!] No RL model found, fallback to pretrained: {cur_model_path}")

    # åˆå§‹åŒ–ç™½æ–¹
    white_model = expert_model if white_strategy=='expert' else None

    for cycle in range(1, num_cycles+1):
        print(f"\n=== ğŸš€ Cycle {cycle}/{num_cycles} å¼€å§‹ ===")
        black_model = load_model(cur_model_path, device)
        # ç™½æ–¹ç­–ç•¥
        if white_strategy=='latest':
            white_model = black_model
        elif white_strategy=='none':
            white_model = None

        # è‡ªå¯¹å¼ˆ
        save_path = f"pkls/self_play_cycle{cycle}.pkl"
        self_play_games(
            model_black=black_model,
            model_white=white_model,
            num_games=games_per_cycle,
            save_path=save_path,
            cycle_index=cycle,
            num_simulations=MCTS_PLAYOUTS
        )
        # è®­ç»ƒ
        print("ğŸ¯ å¼€å§‹è®­ç»ƒé»‘æ–¹æ¨¡å‹...")
        p_loss, v_loss = train_agent(
            model=black_model,
            epochs=epochs_per_cycle,
            data_path=save_path
        )
        # ä¿å­˜
        next_model_path = f"models/model_cycle{cycle}.pth"
        os.makedirs(os.path.dirname(next_model_path), exist_ok=True)
        # torch.save(black_model.state_dict(), next_model_path)
        torch.save({'model_state': black_model.state_dict()}, next_model_path)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°ï¼š{next_model_path}")
        # åŠ¨æ€åˆ‡æ¢
        if white_strategy in ('expert','latest'):
            try:
                from evaluate import evaluate_models
                wins = evaluate_models(
                    model_a=black_model,
                    model_b=white_model,
                    num_games=10,
                    num_simulations=MCTS_PLAYOUTS
                )
                winrate = wins[0]/sum(wins) if sum(wins)>0 else 0
                print(f"[Eval] Black vs White winrate: {winrate:.2f}")
                if winrate > winrate_threshold:
                    print(f"[Swap] èƒœç‡ {winrate:.2f} > {winrate_threshold}ï¼Œåˆ‡æ¢ç™½æ–¹æ¨¡å‹")
                    white_model = black_model
            except ImportError:
                print("[Warning] æ—  evaluate_modelsï¼Œè·³è¿‡åˆ‡æ¢")
        cur_model_path = next_model_path
    print("=== æ‰€æœ‰ Cycle å®Œæˆ ===")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--cycles', type=int, default=4, help='è®­ç»ƒ Cycle æ•°')
    parser.add_argument('--games', type=int, default=GAMES_PER_CYCLE, help='æ¯ Cycle å¯¹å±€æ•°')
    parser.add_argument('--epochs', type=int, default=EPOCHS_PER_CYCLE, help='æ¯ Cycle è®­ç»ƒè½®æ•°')
    parser.add_argument('--strategy', choices=['expert','latest','none'], default='expert', help='ç™½æ–¹æ¨¡å‹ç­–ç•¥')
    parser.add_argument('--threshold', type=float, default=0.6, help='èƒœç‡é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™åˆ‡æ¢ç™½æ–¹æ¨¡å‹')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    cycle_training_loop(
        num_cycles=args.cycles,
        games_per_cycle=args.games,
        epochs_per_cycle=args.epochs,
        device=device,
        white_strategy=args.strategy,
        winrate_threshold=args.threshold
    )
