import torch
import torch.nn as nn
import torch.optim as optim
from replay_buffer import ReplayBuffer


def train_agent(
    model,
    epochs: int = 10,
    batch_size: int = 64,
    data_path: str = 'self_play_data.pkl',
    device: torch.device = None
):
    """
    è®­ç»ƒæ™ºèƒ½ä½“ç½‘ç»œï¼š
      - model:       YourModelClass() å®ä¾‹
      - epochs:      è®­ç»ƒè½®æ•°
      - batch_size:  æ¯è½®å°æ‰¹é‡å¤§å°
      - data_path:   replay buffer ä¿å­˜çš„ pkl è·¯å¾„
      - device:      torch.device('cuda') or torch.device('cpu')
    """
    # 1. å‡†å¤‡è®¾å¤‡
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # 2. åŠ è½½æ•°æ®
    buffer = ReplayBuffer()
    buffer.load(data_path)

    # ğŸ”¥ è°ƒè¯•æ‰“å° buffer å†…å®¹
    buffer_list = buffer.buffer
    print(f"âœ… Buffer åŠ è½½æˆåŠŸï¼Œé•¿åº¦ï¼š{len(buffer)}, buffer ç±»å‹ï¼š{type(buffer_list)}, å‰3å…ƒç´ ç±»å‹ï¼š{[type(buffer_list[i]) for i in range(min(3, len(buffer_list)))]}")

    dataset_size = len(buffer)
    if dataset_size == 0:
        raise RuntimeError(f"ReplayBuffer ä¸ºç©ºï¼š{data_path}")

    # 3. ä¼˜åŒ–å™¨ & æŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    policy_loss_fn = nn.KLDivLoss(reduction='batchmean')
    value_loss_fn = nn.MSELoss()

    # 4. å¼€å§‹è®­ç»ƒ
    for epoch in range(1, epochs + 1):
        model.train()
        total_p_loss = 0.0
        total_v_loss = 0.0

        # æ¯ä¸ª epoch éšæœºåˆ† batch è®­ç»ƒ
        num_batches = max(1, dataset_size // batch_size)
        for _ in range(num_batches):
            # ä» buffer ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªå°æ‰¹æ¬¡
            states, policies, values = buffer.sample(batch_size)
            states = states.to(device)
            policies = policies.to(device)
            values = values.to(device)

            # å‰å‘
            pred_p, pred_v = model(states)
            log_p = nn.LogSoftmax(dim=1)(pred_p)

            # è®¡ç®—ç­–ç•¥ loss å’Œä»·å€¼ loss
            p_loss = policy_loss_fn(log_p, policies)
            v_loss = value_loss_fn(pred_v, values)
            loss = p_loss + v_loss

            # åå‘ + æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_p_loss += p_loss.item()
            total_v_loss += v_loss.item()

        avg_p = total_p_loss / num_batches
        avg_v = total_v_loss / num_batches
        print(f"[Epoch {epoch:02d}/{epochs}] Policy Loss: {avg_p:.4f} | Value Loss: {avg_v:.4f}")

    return avg_p, avg_v


if __name__ == "__main__":
    import argparse
    from network import YourModelClass  # æ›¿æ¢ä¸ºå®é™…æ¨¡å‹ç±»

    parser = argparse.ArgumentParser(description="Train the Gomoku agent.")
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--data_path', type=str, default='self_play_data.pkl')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'])
    parser.add_argument('--model_path', type=str, default=None)
    args = parser.parse_args()

    device = torch.device(args.device if torch.cuda.is_available() or args.device == 'cpu' else 'cpu')
    model = YourModelClass()

    if args.model_path:
        checkpoint = torch.load(args.model_path, map_location=device)
        if isinstance(checkpoint, dict) and 'model_state' in checkpoint:
            model.load_state_dict(checkpoint['model_state'])
            print(f"âœ… ä» checkpoint åŠ è½½æ¨¡å‹å‚æ•°: {args.model_path}")
        else:
            model.load_state_dict(checkpoint)
            print(f"âœ… ä»æƒé‡æ–‡ä»¶åŠ è½½æ¨¡å‹: {args.model_path}")

    # è°ƒç”¨è®­ç»ƒ
    train_agent(
        model=model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        data_path=args.data_path,
        device=device
    )